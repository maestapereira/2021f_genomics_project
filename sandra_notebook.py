# -*- coding: utf-8 -*-
"""SandraWorkNotebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Id_kjQOO4QX3m4PAygRUwMPORAq5Of4_
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os

# import data into dataframe
df_raw = pd.read_csv('raw_data.zip', compression = 'zip').drop(columns=['Unnamed: 0'])
df_feature = pd.read_csv('feature_data.zip', compression = 'zip').drop(columns=['Unnamed: 0'])
df_labels = pd.read_csv('label_data.zip', compression = 'zip').drop(columns=['Unnamed: 0'])

df_raw.head()

df_feature.head()

"""## IGNORE ABOVE"""

! pip install autogluon

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt

from tqdm import tqdm
import torch.nn as nn
import pandas as pd
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data as Data

from tensorflow import keras

from torch.optim import Adam, SGD
from sklearn.metrics import confusion_matrix
from keras.layers import Dense, Bidirectional, Conv1D, MaxPool1D, Flatten, Dropout
from keras.layers import Input
from keras.models import Model
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import BatchNormalization
import keras
from keras.callbacks import EarlyStopping, ModelCheckpoint

from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_fscore_support
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Add

from autogluon.tabular import TabularDataset, TabularPredictor
from sklearn.model_selection import StratifiedShuffleSplit
from tensorflow.keras.layers import SimpleRNN, LSTM

from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/gdrive')

from pathlib import Path

DATA = Path("/gdrive/My Drive/columbia_masters/classes/2021_f/genomics/genomics_project")

# read data
df_raw = pd.read_csv(DATA / 'raw_data.zip', compression = 'zip').drop(columns=['Unnamed: 0'])
df_feature = pd.read_csv(DATA / 'feature_data.zip', compression = 'zip').drop(columns=['Unnamed: 0'])
df_labels = pd.read_csv(DATA / 'label_data.zip', compression = 'zip').drop(columns=['Unnamed: 0'])

# save features and labels in a variable
df_raw = pd.merge(df_feature, df_labels, on='id')
df_raw.head()

df_labels.head()

df_feature.head()

df_raw['n_moa'] = df_labels.drop('id', axis=1).sum(axis=1)
plt.hist(df_raw['n_moa'])
plt.title = 'Number of occurences by number of moa'
plt.xlabel('Number of moa')
plt.ylabel('Number of occurences')

plt.grid(True)
plt.show()

labels_count = df_labels.drop(columns='id').sum(axis=0)
plt.hist(labels_count)
plt.xlabel('Number of occurences')
plt.ylabel('Number of labels')
plt.title = 'Number of labels by number of occurences'
plt.grid(True)
plt.show()

# check how many occurences of each label
np.unique(labels_count, return_counts=True)

"""Remove columns (labels) with less than 15 occurences"""

labels_count = df_labels.drop(columns='id').sum(axis=0)
# minimum label occurences
min_label_occur = 100
# we need to add one because we need to count the id column
drop_labels = list(labels_count[(labels_count<min_label_occur)].index)
drop_labels

# drop on dataset
sel_raw = df_raw.drop(columns=drop_labels)
sel_labels = df_labels.drop(columns=drop_labels)

# re-calculate n_moa
sel_raw['n_moa'] = sel_labels.drop('id', axis=1).sum(axis=1)
plt.hist(sel_raw['n_moa'])
plt.title = 'Number of occurences by number of moa'
plt.xlabel('Number of moa')
plt.ylabel('Number of occurences')

plt.grid(True)
plt.show()

"""Remove rows (samples) with:
* `n_moa > 1`
* `n_moa!= 1`
"""

# keep only rows with one or less moa
zon_moa_idx = np.where(sel_raw['n_moa']>1)[0]
zon_moa_feature = df_feature.drop(index=zon_moa_idx).reset_index(drop=True)
zon_moa_labels = sel_labels.drop(index=zon_moa_idx).reset_index(drop=True)
# keep only rows with one moa
one_moa_idx = np.where(sel_raw['n_moa']!=1)[0]
one_moa_feature = df_feature.drop(index=one_moa_idx).reset_index(drop=True)
one_moa_labels = sel_labels.drop(index=one_moa_idx).reset_index(drop=True)

print(f'All n moa shapes {(df_feature.shape, df_labels.shape)}')
print(f'One or less moa only shapes {(zon_moa_feature.shape, zon_moa_labels.shape)}')
print(f'One moa only shapes {(one_moa_feature.shape, one_moa_labels.shape)}')

"""Separate Training and testing (and validation) data"""

# get train set
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
print(sss)
one_moa_feature['dtype'] = 'test'
one_moa_labels['dtype'] = 'test'
for train_index, test_index in sss.split(one_moa_feature.drop(columns=['id']), one_moa_labels.drop(columns=['id'])):
  one_moa_feature.iloc[train_index, -1] = 'train'
  one_moa_labels.iloc[train_index, -1] = 'train'
  one_moa_feature.iloc[test_index, -1] = 'test'
  one_moa_labels.iloc[test_index, -1] = 'test'

# get test and validation sets
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.7, random_state=42)
print(sss)
for val_index, test_index in sss.split(one_moa_feature.drop(columns=['id'])[one_moa_feature['dtype'] == 'test'], one_moa_labels.drop(columns=['id'])[one_moa_labels['dtype'] == 'test']):
  one_moa_feature.iloc[val_index, -1] = 'val'
  one_moa_labels.iloc[val_index, -1] = 'val'
  one_moa_feature.iloc[test_index, -1] = 'test'
  one_moa_labels.iloc[test_index, -1] = 'test'

one_moa_labels['dtype'].value_counts()

train_labels = one_moa_labels[one_moa_labels.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype']).to_numpy()
train_labels = np.argmax(train_labels, axis=1)
# train_labels = keras.utils.to_categorical(
#     train_labels, dtype='int32'
# )
# train_labels = np.reshape(train_labels, (train_labels.shape[0], train_labels.shape[1], 1)).astype('float')

test_labels = one_moa_labels[one_moa_labels.iloc[:, -1] == 'test'].drop(columns=['id', 'dtype']).to_numpy()
test_labels = np.argmax(test_labels, axis=1)
# test_labels = keras.utils.to_categorical(
#     test_labels, dtype='int32'
# )
# test_labels = np.reshape(test_labels, (test_labels.shape[0], test_labels.shape[1], 1)).astype('float')

val_labels = one_moa_labels[one_moa_labels.iloc[:, -1] == 'val'].drop(columns=['id', 'dtype']).to_numpy()
val_labels = np.argmax(val_labels, axis=1)
# val_labels = keras.utils.to_categorical(
#     val_labels, dtype='int32'
# )
# val_labels = np.reshape(val_labels, (val_labels.shape[0], val_labels.shape[1], 1)).astype('float')

# np.expand_dims(train_features[:,:-1], axis = 2)

train_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype']).reset_index(drop=True)
temp = pd.get_dummies(train_features)
drug_cols = [col for col in temp.columns if col.startswith('drug_')]
drug_int = (temp[drug_cols].idxmax(axis=0))-min(temp[drug_cols].idxmax(axis=0))
train_features['drug'] = drug_int.reset_index(drop=True).astype(int)
train_features['with_drug'] = train_features['with_drug'].astype(int)
train_features.loc[np.where(train_features.dosage=='D1')[0],'dosage']=0
train_features.loc[np.where(train_features.dosage=='D2')[0],'dosage']=1
# train_features = np.expand_dims(train_features.to_numpy(), axis = 2)

test_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'test'].drop(columns=['id', 'dtype']).reset_index(drop=True)
temp = pd.get_dummies(test_features)
drug_cols = [col for col in temp.columns if col.startswith('drug_')]
drug_int = (temp[drug_cols].idxmax(axis=0))-min(temp[drug_cols].idxmax(axis=0))
test_features['drug'] = drug_int.reset_index(drop=True).astype(int)
test_features.loc[np.where(test_features.dosage=='D1')[0], 'dosage']=0
test_features.loc[np.where(test_features.dosage=='D2')[0], 'dosage']=1
test_features['with_drug'] = test_features['with_drug'].astype(int)
# test_features = np.expand_dims(test_features.to_numpy(), axis = 2)

val_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'val'].drop(columns=['id', 'dtype']).reset_index(drop=True)
temp = pd.get_dummies(val_features)
drug_cols = [col for col in temp.columns if col.startswith('drug_')]
drug_int = (temp[drug_cols].idxmax(axis=0))-min(temp[drug_cols].idxmax(axis=0))
val_features['drug'] = drug_int.reset_index(drop=True).astype(int)
val_features.loc[np.where(val_features.dosage=='D1')[0], 'dosage']=0
val_features.loc[np.where(val_features.dosage=='D2')[0], 'dosage']=1
val_features['with_drug'] = val_features['with_drug'].astype(int)
# val_features = np.expand_dims(val_features.to_numpy(), axis = 2)

# train_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype']).reset_index(drop=True)
# temp = pd.get_dummies(train_features)
# drug_cols = [col for col in temp.columns if col.startswith('drug_')]
# drug_col = (temp[drug_cols].idxmax(axis=0))-min(temp[drug_cols].idxmax(axis=0))
# train_features['drug'] = drug_col.reset_index(drop=True).astype(int)
# train_features.head()

# temp = pd.get_dummies(one_moa_feature[one_moa_feature.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype']))
# drug_cols = [col for col in temp.columns if col.startswith('drug_')]
# drug_col = (temp[drug_cols].idxmax(axis=0))-min(temp[drug_cols].idxmax(axis=0))
# train_features['drug'] = drug_col.reset_index(drop=True).astype(int)
# train_features.head()

# train_features.shape

# (train_features[drug_cols].idxmax(axis=0))

# drug_col = (temp[drug_cols].idxmax(axis=0))-min(temp[drug_cols].idxmax(axis=0))
# drug_col.reset_index(drop=True).astype(int)

# pd.get_dummies(one_moa_feature[one_moa_feature.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype'])).columns

print(f'Train, test, val labels {(train_labels.shape, test_labels.shape, val_labels.shape)}')
print(f'Train, test, val features {(train_features.shape, test_features.shape, val_features.shape)}')

"""### Prediction model
* 1 to 4 convolutional neural network (CNN) layers
* 1 to 2 bidirectional recurrent neural network (RNN) layers
* 1 to 2 fully connected (FC) layers, in a global architecture layout CNN-RNN-FC
"""

train_features['label'] = train_labels

save_path = 'agModels-predictClass'  # specifies folder to store trained models
predictor = TabularPredictor(label='label', path=save_path).fit(train_features)

predictor = TabularPredictor.load(save_path)
y_pred = predictor.predict(test_features)
print("Predictions:  \n", y_pred)
perf = predictor.evaluate_predictions(y_true=pd.Series(test_labels), y_pred=y_pred, auxiliary_metrics=True)



train_features[0].shape

def bi_LSTM(x_train,y_train):
    inputs=Input(shape=(x_train[0].shape), name='inputs')
    
    dense1 = Dense(32, activation='relu')(inputs)
    biLSTM1 = Bidirectional(LSTM(128))(dense1)
    drop1 = Dropout(0.3)(biLSTM1)
    bn1 = BatchNormalization()(drop1)
    dense2 = Dense(64, activation='relu')(bn1)
    drop2 = Dropout(0.3)(dense2)
    bn2 = BatchNormalization()(drop2)
    main_output = Dense(1, activation='softmax')(bn2)

    model = Model(inputs= inputs, outputs=main_output, name='BiLSTM_Model')
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])

inputs=Input(shape=(train_features[0].shape), name='inputs')

dense1 = Dense(32, activation='relu')(inputs)
con1 = Convolution1D(64, (6), activation='relu', name='conv1d_1')(inputs)
birnn1 = Bidirectional(SimpleRNN(128))(con1)
drop1 = Dropout(0.3)(birnn1)
bn1 = BatchNormalization()(drop1)
dense2 = Dense(64, activation='relu')(bn1)
drop2 = Dropout(0.3)(dense2)
bn2 = BatchNormalization()(drop2)
main_output = Dense(1, activation='softmax')(bn2)

model = Model(inputs= inputs, outputs=main_output, name='BiRNN_Model')
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])
model.summary()

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
history1 = model1.fit(convert_to_tensor(train_features, dtype=tf.float64), convert_to_tensor(train_labels, dtype=tf.float64),
                      epochs=10, batch_size=64,
                      validation_data=(convert_to_tensor(val_features, dtype=tf.float64), convert_to_tensor(val_labels, dtype=tf.float64)),
                      callbacks=[es])

def network(X_train,y_train):
    im_shape=(X_train.shape[1],1)
    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')
    
    # START my code
    # convolution layer: 
    #   filters=64, (last dimension of output from this layer)
    #   kernel_size=(6), (2nd dimension of output from this layer is previous layers'-6+1, which means kernel size has to be 6)
    conv1d_1 = Convolution1D(64, (6), activation='relu', name='conv1d_1')(inputs_cnn)
    # batch normalization
    batch_normalization = BatchNormalization(name='batch_normalization')(conv1d_1)
    # max pooling:
    #   pool_size and strides=2, (2nd output dimension from this layer is 1/2 from 2nd dimension of input to this layer)
    #   padding="same" rounds up division of output dimension
    max_pooling1d = MaxPool1D(pool_size=2, strides=2, padding='same', name='max_pooling1d')(batch_normalization)
    # convolution layer: 
    #   filters=128, (last dimension of output from this layer)
    #   kernel_size=(3), (2nd dimension of output from this layer is previous layers'-3+1, which means kernel size has to be 3)
    conv1d_2 = Convolution1D(128, (3), name='conv1d_2')(max_pooling1d)
    # batch normalization
    batch_normalization_1 = BatchNormalization(name='batch_normalization_1')(conv1d_2)
    # max pooling:
    #   pool_size and strides=2, (2nd output dimension from this layer is 1/2 from 2nd dimension of input to this layer)
    #   padding="same" rounds up division of output dimension
    max_pooling1d_1 = MaxPool1D(pool_size=2, strides=2, padding='same', name='max_pooling1d_1')(batch_normalization_1)
    # flattening
    flatten = Flatten(name='flatten')(max_pooling1d_1)
    # fully connected layers
    dense = Dense(64, activation='relu', name='dense')(flatten)
    dense_1 = Dense(32, activation='relu', name='dense_1')(dense)
    main_output = Dense(1, activation='softmax', name='main_output')(dense_1)
    # END my code
    
    model = Model(inputs= inputs_cnn, outputs=main_output, name='model_cnn')
    model.compile(optimizer='adam', loss="categorical_crossentropy",metrics = ['accuracy'])

    return(model)

model1 = network(train_features, train_labels)
print(model1.summary())

from tensorflow import convert_to_tensor
import tensorflow as tf

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
history1 = model1.fit(convert_to_tensor(train_features, dtype=tf.float64), convert_to_tensor(train_labels, dtype=tf.float64),
                      epochs=10, batch_size=64,
                      validation_data=(convert_to_tensor(val_features, dtype=tf.float64), convert_to_tensor(val_labels, dtype=tf.float64)),
                      callbacks=[es])

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
history1 = model1.fit(train_features, train_labels,
                      epochs=10, batch_size=64,
                      validation_data=(val_features, val_labels),
                      callbacks=[es])

# Implement your code here:
# Hint: use an embedding layer to project your sequence data to a higher dimension,
# and use the diagram above for some ideas on layers to include in your model.
# Remember to compile your model after designing it.

x_input = Input(shape=(train_features.shape[1],train_features.shape[2]))
# embedding layer
e1 = Convolution1D(1, 128)(x_input)
# bidirection Dense layer
# b1 = Bidirectional(Dense(64, activation='relu', ))(e1)

b1 = Bidirectional(SimpleRNN(
    4,
    activation='relu',
    use_bias=True,
    kernel_initializer="glorot_uniform",
    recurrent_initializer="orthogonal",
    bias_initializer="zeros",
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=True,
    return_state=True,
    go_backwards=True,
    stateful=False,
    unroll=False
))(e1)
# add some dropout to speed it up/improve model
x = Dropout(0.3)(b1)

# softmax fully connected layer
x_output = Dense(300, activation='softmax')(x)

model1 = Model(inputs=x_input, outputs=x_output)
model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model1.summary()





# Evaluation function
# added disp_labels as input for confusion matrix
def evaluate_model(history, X_test, y_test, model, disp_labels=None):
    scores = model.evaluate((X_test),y_test, verbose=0)
    print("Accuracy: %.2f%%" % (scores[1]*100))
    
    print(history)
    fig1, ax_acc = plt.subplots()
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Model - Accuracy')
    plt.legend(['Training', 'Validation'], loc='lower right')
    plt.show()
    
    fig2, ax_loss = plt.subplots()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Model- Loss')
    plt.legend(['Training', 'Validation'], loc='upper right')
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.show()
    
    prediction_proba = model.predict(X_test)
    prediction = np.argmax(prediction_proba, axis=1)

    # changed confusion matrix for aesthetics
    cnf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, prediction,
                                                         cmap='Blues',
                         display_labels=disp_labels)

def network(X_train,y_train):
    im_shape=(X_train.shape[1],1)
    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')

    # convolution layer: 
    #   filters=64, (last dimension of output from this layer)
    #   kernel_size=(6), (2nd dimension of output from this layer is previous layers'-6+1, which means kernel size has to be 6)
    conv1d_1 = Convolution1D(64, (6), activation='relu', name='conv1d_1')(inputs_cnn)
    # batch normalization
    batch_normalization = BatchNormalization(name='batch_normalization')(inputs_cnn)
    # max pooling:
    #   pool_size and strides=2, (2nd output dimension from this layer is 1/2 from 2nd dimension of input to this layer)
    #   padding="same" rounds up division of output dimension
    max_pooling1d = MaxPool1D(pool_size=2, strides=2, padding='same', name='max_pooling1d')(batch_normalization)
    # convolution layer: 
    #   filters=128, (last dimension of output from this layer)
    #   kernel_size=(3), (2nd dimension of output from this layer is previous layers'-3+1, which means kernel size has to be 3)
    conv1d_2 = Convolution1D(128, (3), name='conv1d_2')(max_pooling1d)
    # batch normalization
    batch_normalization_1 = BatchNormalization(name='batch_normalization_1')(max_pooling1d)
    # max pooling:
    #   pool_size and strides=2, (2nd output dimension from this layer is 1/2 from 2nd dimension of input to this layer)
    #   padding="same" rounds up division of output dimension
    max_pooling1d_1 = MaxPool1D(pool_size=2, strides=2, padding='same', name='max_pooling1d_1')(batch_normalization_1)
    # flattening
    flatten = Flatten(name='flatten')(max_pooling1d_1)
    # fully connected layers
    dense = Dense(64, activation='relu', name='dense')(flatten)
    dense_1 = Dense(32, activation='relu', name='dense_1')(dense)
    main_output = Dense(3, activation='softmax', name='main_output')(dense_1)
    # END my code
    
    model = Model(inputs= inputs_cnn, outputs=main_output, name='model_cnn')
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics = ['accuracy'])

    return(model)

model1 = network(train_features, train_labels)
print(model1.summary())

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
history1 = model1.fit(train_features, train_labels,
                      epochs=10, batch_size=64,
                      validation_data=(val_features, val_labels),
                      callbacks=[es])

np.argmax(df_labels.drop('id'))



# train_labels = one_moa_labels[one_moa_labels.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype']).to_numpy()
# train_labels = np.argmax(train_labels, axis=1)
# train_labels = keras.utils.to_categorical(
#     train_labels, dtype='int32'
# )
# # train_labels = np.reshape(train_labels, (train_labels.shape[0], train_labels.shape[1], 1)).astype('float')

# test_labels = one_moa_labels[one_moa_labels.iloc[:, -1] == 'test'].drop(columns=['id', 'dtype']).to_numpy()
# test_labels = np.argmax(test_labels, axis=1)
# test_labels = keras.utils.to_categorical(
#     test_labels, dtype='int32'
# )
# # test_labels = np.reshape(test_labels, (test_labels.shape[0], test_labels.shape[1], 1)).astype('float')

# val_labels = one_moa_labels[one_moa_labels.iloc[:, -1] == 'val'].drop(columns=['id', 'dtype']).to_numpy()
# val_labels = np.argmax(val_labels, axis=1)
# val_labels = keras.utils.to_categorical(
#     val_labels, dtype='int32'
# )
# # val_labels = np.reshape(val_labels, (val_labels.shape[0], val_labels.shape[1], 1)).astype('float')

# train_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'train'].drop(columns=['id', 'dtype'])
# test_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'test'].drop(columns=['id', 'dtype'])
# val_features = one_moa_feature[one_moa_feature.iloc[:, -1] == 'val'].drop(columns=['id', 'dtype'])

# Implement your code here:
# Hint: use an embedding layer to project your sequence data to a higher dimension,
# and use the diagram above for some ideas on layers to include in your model.
# Remember to compile your model after designing it.

x_input = Input(shape=(train_labels.shape[1],))
# embedding layer
e1 = Convolution1D(1, 128)(x_input)
# bidirection LSTM layer
b1 = Bidirectional(Dense(64))(e1)
# add some dropout to speed it up/improve model
x = Dropout(0.3)(b1)

# softmax fully connected layer
x_output = Dense(300, activation='softmax')(x)

model1 = Model(inputs=x_input, outputs=x_output)
model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model1.summary()

model1 = network(data_train, label_train)
print(model1.summary())

# Train you model
# add early stopping to prevent overfitting
es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
history1 = model1.fit(data_train, label_train,
                      epochs=10, batch_size=64,
                      validation_data=(data_val, label_val),
                      callbacks=[es])

# Print result and plot accuracy and loss
evaluate_model(history1, data_test, label_test, model1)
y_pred1 = model1.predict(data_test)



#Add your code to define a residual block below, as shown in the diagram above
def residual_block(data, filters, d_rate, batch_normalize=False):
  """
  _data: input
  _filters: convolution filters
  _d_rate: dilation rate
  """

  #Add your layers here
  # layer 1
  if (batch_normalize == True):
    data = BatchNormalization()(data)
  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', activation = 'relu')(data)

  # layer 2: bottleneck convolution
  if (batch_normalize == True):
    conv1 = BatchNormalization()(conv1)
  conv2 = Conv1D(filters, 3, padding='same', activation = 'relu')(conv1)

  # skip connection
  x = Add()([conv2, data])

  return x

#Insert your code before and after the residual networks called below

# input
x_input = Input(shape=(train_labels.shape[1], train_labels.shape[2]))

#initial conv
conv = Conv1D(128, 1, padding='same')(x_input) 

# per-residue representation
res1 = residual_block(conv, 128, 2)
res2 = residual_block(res1, 128, 3)

# Max pooling
x = MaxPool1D(3)(res2)
x = Dropout(0.5)(x)

# softmax classifier
x = Flatten()(x)
x_output = Dense(300, activation='softmax')(x)

#Compile your model
model2 = Model(inputs=x_input, outputs=x_output)
model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model2.summary()

# , kernel_regularizer=l2(0.0001)

# Train you model
# add early stopping to prevent overfitting
es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
history2 = model2.fit(
    x = train_features, 
    y = train_labels,
    epochs=10, batch_size=256,
    validation_data = (val_features, val_labels),
    callbacks=[es])

